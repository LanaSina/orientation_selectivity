---
title: "filter_analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Setup 

```{r}
#install.packages("gplots")
library(gplots) 
#install.packages("tidyr")
library(tidyr) 
require(ggplot2)
#install.packages("reshape2")
library(reshape2)
library(ggplot2)
library(tidyr)
#install.packages("plotly")
library(plotly)

folderName = "/Users/lana/Development/SNET_data/"
```

Read errors for whole velocity prediction
```{r}
experiment = "KITCHEN/"
fileName = paste(folderName,"velocity_whole_prediction/pixels/", experiment, "prediction_weights.csv", sep = "")

print(fileName)
df = read.csv(fileName)
pix_weights = df

fileName = paste(folderName,"velocity_whole_prediction/filters/", experiment, "contrast/", "prediction_weights.csv", sep = "")
print(fileName)
df = read.csv(fileName)
f_weights = df

plot(pix_weights$step, pix_weights$error, type="l")
points(f_weights$step, f_weights$error, type="l", col="red")
```



Read predicition for velocity + pixel

```{r}
experiment = "KITTI/x_138_y_34/"
fileName = paste(folderName,"velocity_prediction/filters/", experiment, "prediction_weights.csv", sep = "")

print(fileName)
df = read.csv(fileName)
weights = df
weights$p = weights$weight_value / weights$weight_age
print(weights$weight_age)
print(weights$p)

```


Read prediction matrix for single pixel predictions

useful functions
```{r}

#calculate average influence by distance in same grayscale for several neurons
averageByDistance = function(side, ids){
  #just to calculate maxDist
  maxDist = sqrt(2*side*side)
  n = floor(maxDist+1)
  pd = rep(0,n)

  for(nId in ids){
    gscale = floor(nId/nInGrayscale)
    avMatrix = averageInfluence(side, nId,gscale,gscale)
    
    k = nId%%nInGrayscale 
    y = floor(k/side)
    x = k%%side
    pointData = data.frame(x=x, y=y)
    temp = predictabilityByDistance(side, avMatrix,pointData)
    pd = pd + temp
  }
  
  pd = pd/length(ids)
  return(pd)
}


#print influence relative to distance
predictabilityByDistance = function(side, predictionMatrix, pointData){
  maxDist = sqrt(2*side*side) 
  n = floor(maxDist+1)
  avPredictions = rep(0,n)
  pointsCount = rep(0,n)
  
  rows = length(predictionMatrix[,1]) 
  cols = length(predictionMatrix[1,]) 
  
  for(i in 1:rows){
    for(j in 1:cols){
      d = sqrt((pointData$x - j)^2 + (pointData$y - i)^2)
      avPredictions[floor(d)] = avPredictions[floor(d)]+predictionMatrix[i,j] 
      pointsCount[floor(d)] = pointsCount[floor(d)] + 1
    }
  }
  
  avPredictions = avPredictions/pointsCount
  avPredictions[is.na(avPredictions)] <- 0
  return(avPredictions)
}


#average on several grayscales
averageInfluence = function(side, nId, scaleStart, scaleStop){
  sumMatrix = matrix(data=rep(0,side*side), nrow=side, ncol=side, byrow = TRUE)
  sub = subset(weights, from_neuron==nId)
  
  for(toGrayscale in scaleStart:scaleStop){
    #cut by grayscale
    scale = subset(sub, (to_neuron>=(nInGrayscale*toGrayscale)) &
                     (to_neuron<(nInGrayscale*(toGrayscale+1)))
    )
    startId =  nInGrayscale*toGrayscale
    
    if(length(scale$to_neuron)!=0){
      scale = subset(scale, weight_age!=0)
      orderedScale = scale[order(scale$to_neuron),]
      
      #fill missing values
      zeroes = data.frame(to_neuron=c(startId:(startId + nInGrayscale -1)))#, p=rep(0,nInGrayscale)
      filled <- merge(orderedScale, zeroes, all=TRUE)
      filled[is.na(filled)] <- 0
      
      #cut by position
      field = matrix(data=filled$p, nrow=side, ncol=side, byrow = TRUE)
      field = field[1:side,]
      
      sumMatrix = sumMatrix+field
    }
  }
  
  return(sumMatrix/(scaleStop-scaleStart+1.0))
}

```

Read data
```{r}
 
experiment = "CROPPED_OSWALD_4/"
fileName = paste(folderName,"single_pixel_prediction/",  experiment, "prediction_weights.csv", sep = "")

print(fileName)
df = read.csv(fileName)
weights = df
weights$p = weights$weight_value / weights$weight_age
#sanity check
length(subset(weights,p>1)$p)

side = 50 #image size
nInGrayscale = side*side
ids = unique(weights$from_neuron)

```


calculate prediction = f(distance), save, and display prediction = f(distance)

```{r}
#p = f(d) for all neurons
avPd = averageByDistance(side, ids)
plot(avPd, type = "l",
     xlab = "Distance (pixels)", ylab = "Predictive power",
     ylim = c(0,1)
     )
filename =  paste(folderName,"single_pixel_prediction/", experiment, "averagedPredictionByDistance.csv", sep="")
print(filename)
write.csv(avPd, filename)
```


Save the plot as clean pdf

```{r}
filename = paste(folderName, "single_pixel_prediction/", experiment, "single_pixel_greyscale.pdf", sep = "")
print(filename)
pdf(filename, width=4, height=4) 
plot(avPd, type = "l",
     xlab = "Distance (pixels)", ylab = "Predictive power",
     ylim = c(0,1)
     )
# Close the pdf file
dev.off() 

```


Make surface plot

```{r}

configuration = "single_pixel_prediction/CROPPED_FPSI/t1_4/"
filename =  paste(folderName, configuration, "averagedPredictionByDistance.csv", sep="")
print(filename)
t1 = read.csv(filename)

configuration = "single_pixel_prediction/CROPPED_FPSI/t5_4/"
filename =  paste(folderName, configuration, "averagedPredictionByDistance.csv", sep="")
t5 = read.csv(filename)

configuration = "single_pixel_prediction/CROPPED_FPSI/t10_4/"
filename =  paste(folderName, configuration, "averagedPredictionByDistance.csv", sep="")
t10 = read.csv(filename)

configuration = "single_pixel_prediction/CROPPED_FPSI/t20_4/"
filename =  paste(folderName, configuration, "averagedPredictionByDistance.csv", sep="")
t20 = read.csv(filename)

d = length(t1$x)
z = matrix(nrow = d, ncol = 4)
z[,1] = t1$x
z[,2] = t5$x
z[,3] = t10$x
z[,4] = t20$x
#ply = 
plot_ly(x=c(1,5,10,20,60),y=c(0:(d-1)),z=z, type="surface")
```


Process data for filter-based prediction

Useful functions

```{r}

allOrientationsByDistance = function(weights, direction, filterPosition){
  maxSize = max(weights[[direction]])
  horizontalW = subset(weights, filterId == 0)
  verticalW = subset(weights, filterId == 1)
  randomW = subset(weights, filterId == 2)
  fullW = subset(weights, filterId == 3)
  np = filterPosition
  
  verticalWd = meanByDistance(verticalW, direction, np, maxSize)
  horizontalWd = meanByDistance(horizontalW, direction, np, maxSize)
  fullWd = meanByDistance(fullW,"neuron_x", np, maxSize)
  randomWd = meanByDistance(randomW, direction, np, maxSize)
  
  result = data.frame(distance = verticalWd$distance,
                      horizontal = horizontalWd$p,
                      vertical = verticalWd$p,
                      full = fullWd$p, random = randomWd$p)
  
  return(result)
}

#param axis: string "neuron_x" or "neuron_y"
#param origin = middle of the filter for that axis
meanByDistance = function(subWeights, axis, origin, maxSize){
  #maxSize = length(subWeights$p)
  subWeights = sortByDistance(subWeights, axis, origin)
  
  instances = rep(0, maxSize)
  result = data.frame(distance = c(0:(maxSize-1)), p = rep(0, maxSize))
  for(i in 1:maxSize){
    sub = subset(subWeights, distance==i-1)
    instances[i] = instances[i] + length(sub$p)
    result$p[i] = result$p[i] + sum(sub$p)
  }
  
  result$p = result$p/instances
  result$p[is.na(result$p)] <- 0
  
  return(result)
}

sortByDistance = function(weights, axis, origin){
  result = weights
  result$distance = abs(result[[axis]] - origin)
  result = result[order(result$distance),]
  return(result)
}

plotProbas = function(probas){
  
  plot(probas$distance, probas$vertical, type = "l",
       ylim = c(0,1), xlim = c(0,50),
       xlab = "Distance (pixels)", ylab = "Predictive power"
       )
  points(probas$distance, probas$horizontal, type = "l", col = "red")
  points(probas$distance, probas$full, type = "l", col = "blue")
  points(probas$distance, probas$random, type = "l", col = "green")
  legend(25, 1, legend=c("Vertical filter", "Horizontal filter", "Full filter", "Random filter"),
       col=c("black", "red", "blue", "green"), lty=1, cex=0.8)
}

```

Read one set of filters
```{r}
d = "h"

configuration = "filter_to_filter_prediction/contrast/DEBUG/t1"
filterFolder = "x_22_y_12/"
direction = "neuron_x"
subfolder = "/horizontal/"
if(d=="v"){
  direction = "neuron_y"
  subfolder = "/vertical/"
}
experiment = paste(folderName,configuration,subfolder,filterFolder,sep = "")
fileName = paste(experiment, "filter_weights.csv", sep = "")
print(fileName)
df = read.csv(fileName)
weights = df
weights$p = weights$value / weights$age
weights$p[is.na(weights$p)] <- 0

#sanity check
maxAge = max(weights$age)
print(maxAge)
# minAge = min(weights$age)
# print(minAge)
#length(subset(weights,age>0)$p)


fileName = paste(experiment, "filters.csv", sep = "")
#print(fileName)
df = read.csv(fileName)
filters = df
```


Plot weights depending on distance

```{r}

horizontalW = subset(weights, filterId == 0)
verticalW = subset(weights, filterId == 1)
randomW = subset(weights, filterId == 2)
fullW = subset(weights, filterId == 3)

fileName = paste(experiment, "/filter_configuration.csv", sep = "")
print(fileName)
df = read.csv(fileName)
print(df)
conf = df
if(direction=="neuron_x"){
  np = conf$fx[1] + 1 #middle of filter
} else {
  np = conf$fy[1] + 1
}

maxSize = max(weights[[direction]])


verticalWd = meanByDistance(verticalW, direction, np, maxSize)
horizontalWd = meanByDistance(horizontalW, direction, np, maxSize)
fullWd = meanByDistance(fullW,"neuron_x", np, maxSize)
randomWd = meanByDistance(randomW, direction, np, maxSize)

l = max(horizontalW[[direction]])
allProbas = data.frame(distance = c(0:(l-1)), vertical = rep(0, l), horizontal = rep(0, l),
                       random = rep(0, l), full = rep(0, l))
allProbas$distance = verticalWd$distance
allProbas$vertical = verticalWd$p
allProbas$horizontal = horizontalWd$p
allProbas$full = fullWd$p
allProbas$random = randomWd$p

plotProbas(probas = allProbas)

#plot(verticalW$neuron_y,verticalW$p, type = "l")
#plot(verticalWd$distance, verticalWd$p, type = "l")
```

Compare single pixel and filters

```{r}
configuration = "single_pixel_prediction/CROPPED_OSWALD_4/"
filename =  paste(folderName, configuration, "averagedPredictionByDistance.csv", sep="")
print(filename)
pixel_p = read.csv(filename)


configuration = "filter_prediction/greyscale/OSWALD_20FPS/t1"
subfolder = "/horizontal/"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
print(filename)
filter_h = read.csv(filename)

subfolder = "/vertical/"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
print(filename)
filter_v = read.csv(filename)

plot(pixel_p$x, type = "l")
points(filter_v$vertical, type = "l", col="red")
points(filter_h$horizontal, type = "l", col="blue")
```

Read all filters

```{r}
#h or v
d = "v"
configuration = "filter_to_filter_prediction/contrast/FPSI/t1" #OSWALD_20FPS

direction = "neuron_x"
subfolder = "/horizontal/"
if(d=="v"){
  direction = "neuron_y"
  subfolder = "/vertical/"
}

folders <- list.dirs(path=paste(folderName, configuration, subfolder, sep=""),
                    full.names=TRUE, recursive=FALSE)
allWeights = list()
allFilterPositions = list()
i = 0
for(experiment in folders){
  fileName = paste(experiment, "/filter_configuration.csv", sep = "")
  df = read.csv(fileName)
  conf = df
  
  # if(conf$grayscale==-1){
  #   next()
  # }
  if(direction=="neuron_x"){
    np = conf$fx[1] + 1 #middle of filter
  } else {
    np = conf$fy[1] + 1
  }
  
  i = i+1
  
  fileName = paste(experiment, "/filter_weights.csv", sep = "")
  print(fileName)
  df = read.csv(fileName)
  weights = df
  
  maxAge = max(weights$age)
  print(maxAge)

  weights$p = weights$value / weights$age
  weights$p[is.na(weights$p)] <- 0
  allWeights[[i]] = weights
  
  allFilterPositions[[i]] = np
}

```

Aggreagte and plot weights depending on distance

```{r}
l = max(allWeights[[1]][[direction]])
allProbas = data.frame(distance = c(0:(l-1)), vertical = rep(0, l), horizontal = rep(0, l),
                       random = rep(0, l), full = rep(0, l))

n = length(allWeights)

#make lists
vertical = matrix(0,l,n)
horizontal = matrix(0,l,n)
full = matrix(0,l,n)
random = matrix(0,l,n)

activations = rep(0, 2)
for(i in c(1:n)){
  temp = allOrientationsByDistance(allWeights[[i]], direction, allFilterPositions[[i]])
  vertical[,i] = temp$vertical
  horizontal[,i] = temp$horizontal
  full[,i] = temp$full
  random[,i] = temp$random
  
  if(!is.na(temp$vertical[1])){
    activations[1] = activations[1] + 1
  }
  if(!is.na(temp$horizontal[1])){
    activations[2] = activations[2] + 1
  }
  
  plotProbas(temp)
}

allProbas$distance = temp$distance
allProbas$vertical = rowMeans(vertical) #rowSums(vertical)/activations[1]
#standard error = sd / sqrt(n)
allProbas$vertical_sd = apply(vertical,1,sd)/sqrt(n)
allProbas$horizontal = rowMeans(horizontal)#rowSums(horizontal)/activations[2]
allProbas$horizontal_sd = apply(horizontal,1,sd)/sqrt(n)
allProbas$full = rowMeans(full)
allProbas$full_sd = apply(full,1,sd)/sqrt(n)
allProbas$random = rowMeans(random)
allProbas$random_sd = apply(random,1,sd)/sqrt(n)

print(mean(allProbas$vertical[1:50]))
print(mean(allProbas$horizontal[1:50]))
print(mean(allProbas$random[1:50]))
print(mean(allProbas$full[1:50]))
plotProbas(allProbas)

```

Save data

```{r}
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
print(filename)
write.csv(allProbas, filename)

```

Save figure as pdf 
```{r}

filename = paste(folderName, configuration, subfolder, "filters_sem.pdf", sep = "")
print(filename)
pdf(filename, width=4, height=3)

#10 times series are aggregated
n_samples = 10 
sem_factor = sqrt(n_samples)
ggplot(allProbas, aes(distance)) +
       geom_line(aes(y = vertical, color = "Vertical")) +
       geom_ribbon(aes(ymin = vertical - vertical_sd/sem_factor,
                       ymax = vertical + vertical_sd/sem_factor), alpha = 0.2) +
       geom_line(aes(y = horizontal, color = "Horizontal")) +
       geom_ribbon(aes(ymin = horizontal - horizontal_sd/sem_factor,
                       ymax = horizontal + horizontal_sd/sem_factor), alpha = 0.2) +
       geom_line(aes(y = random, color = "Random")) +
       geom_ribbon(aes(ymin = random - random_sd/sem_factor,
                       ymax = random + random_sd/sem_factor), alpha = 0.2) +
       geom_line(aes(y = full, color = "Full")) +
       geom_ribbon(aes(ymin = full - full_sd/sem_factor,
                       ymax = full + full_sd/sem_factor), alpha = 0.2) +
       labs(x="Distance (pixels)", y="Predictive power", colour = "Filter type") +
       xlim(0, 50) + ylim(0,1) #+
       #theme(axis.title.x = element_text(size = 10))

# Close the pdf file
dev.off()

```

Read aggregated data
```{r}
configuration = "filter_prediction/greyscale/KITTI/t1" #OSWALD_20FPS
subfolder = "/horizontal/"

filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
print(filename)
allProbas = read.csv(filename)
```

Read aggregated data and plot as f(t)

```{r}
subfolder = "/vertical/"
configuration = "filter_prediction/greyscale/OSWALD_20FPS/t1"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
t1 = read.csv(filename)

configuration = "filter_prediction/greyscale/OSWALD_20FPS/t5"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
t5 = read.csv(filename)

configuration = "filter_prediction/greyscale/OSWALD_20FPS/t20"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
t20 = read.csv(filename)

configuration = "filter_prediction/greyscale/OSWALD_20FPS/t40"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
t40 = read.csv(filename)

configuration = "filter_prediction/greyscale/OSWALD_20FPS/t60"
filename =  paste(folderName, configuration, subfolder, "aggregated_probabilities.csv", sep="")
t60 = read.csv(filename)

meanByOrientation = function(probas, t){
  v = mean(probas$vertical)
  #get the sem from aggregated variance
  v_sd = sqrt(sum(probas$vertical_sd^2))
  h = mean(probas$horizontal)
  h_sd = sqrt(sum(probas$horizontal_sd^2))
  f = mean(probas$full)
  f_sd = sqrt(sum(probas$full_sd^2))
  r = mean(probas$random)
  r_sd = sqrt(sum(probas$random_sd^2))
  d = data.frame(t = t, v = v, v_sd = v_sd,
                 h = h, h_sd = h_sd,
                 f = f, f_sd = f_sd,
                 r = r, r_sd = r_sd
                 )
  return(d)
}


# plot(t1$distance, t1$vertical, type="l", ylim = c(0,1))
# points(t5$distance, t5$vertical, type="l", col="red")
# points(t20$distance, t20$vertical, type="l", col="green")
# points(t40$distance, t40$vertical, type="l", col="blue")

d = 50
z = matrix(nrow = d, ncol = 5)
z[,1] = t1[1:d,]$vertical
z[,2] = t5[1:d,]$vertical
z[,3] = t20[1:d,]$vertical
z[,4] = t40[1:d,]$vertical
z[,5] = t60[1:d,]$vertical
#ply = 
plot_ly(x=c(1,5,20,40,60),y=c(0:49),z=z, type="surface")
#orca(ply, file = paste(folderName, "filter_prediction/greyscale/OSWALD_20FPS/3d.png", sep=""))

#distance window
d = 20
timeP = rbind(meanByOrientation(t1[1:d,],1),
              meanByOrientation(t5[1:d,],5),
              meanByOrientation(t20[1:d,],20),
              meanByOrientation(t40[1:d,],40),
              meanByOrientation(t60[1:d,],60)
              )

#10 times series were originally aggregated
n_samples = d*10 
sem_factor = sqrt(n_samples)
ggplot(timeP, aes(t)) +
       geom_line(aes(y = v, color = "Vertical")) +
       geom_ribbon(aes(ymin = v - v_sd/sem_factor,
                       ymax = v + v_sd/sem_factor), alpha = 0.2) +
       geom_line(aes(y = h, color = "Horizontal")) +
       geom_ribbon(aes(ymin = h - h_sd/sem_factor,
                       ymax = h + h_sd/sem_factor), alpha = 0.2) +
       # geom_line(aes(y = r, color = "Random")) +
       # geom_ribbon(aes(ymin = r - r_sd,
       #                 ymax = r + r_sd), alpha = 0.2) +
       # geom_line(aes(y = f, color = "Full")) +
       # geom_ribbon(aes(ymin = f - f_sd,
       #                 ymax = f + f_sd), alpha = 0.2) +
       labs(x="Predicted time step", y="Predictive power", colour = "Filter type")

```


save plot
```{r}

filename = paste(folderName, "filter_prediction/greyscale/OSWALD_20FPS/prediction_ft_vertical.pdf", sep = "")
print(filename)
pdf(filename, width=8, height=4)
n_samples = d*10 
sem_factor = sqrt(n_samples)
ggplot(timeP, aes(t)) +
       geom_line(aes(y = v, color = "Vertical")) +
       geom_ribbon(aes(ymin = v - v_sd/sem_factor,
                       ymax = v + v_sd/sem_factor), alpha = 0.2) +
       geom_line(aes(y = h, color = "Horizontal")) +
       geom_ribbon(aes(ymin = h - h_sd/sem_factor,
                       ymax = h + h_sd/sem_factor), alpha = 0.2) +
       labs(x="Predicted frame", y="Predictive power", colour = "Filter type")
# Close the pdf file
dev.off()

```


Drafts 
```{r}
plot(verticalW[[direction]], verticalW$p, type = "l", ylim = c(0,1))
points(horizontalW[[direction]], horizontalW$p, type = "l", col = "red")
points(fullW[[direction]], fullW$p, type = "l", col = "blue")
randomWd = meanByDistance(randomW, direction, np)
points(randomW[[direction]], randomW$p, type = "l", col = "green")
#abline(v=np)
```

```{r}
plot(x=verticalW[[direction]], y=verticalW$p,type = "l")
points(x=horizontalW[[direction]], y=horizontalW$p,type = "l", col = "red")
points(randomW[[direction]], randomW$p, type = "l", col = "green")
#abline(v=301)
```


Plot weights of horizontal/vertical filters

```{r}
#select weights of horizontal/vertical filters
#all "majority vertical" filters
vWeights = data.frame(filterId=NULL, p=NULL)
for(i in c(1:length(weights$filterId))){
  fId = weights$filterId[i]
  sub = subset(filters, filterId==fId)
  if((sub$x[1]==sub$x[2]) 
     || (sub$x[1]==sub$x[3])
     || (sub$x[2]==sub$x[3])
  ){
    if((sub$y[1]!=sub$y[2]) #exclude ambiguous orientations
       & (sub$y[1]!=sub$y[3])
       & (sub$y[2]!=sub$y[3])
    ){
      weight = weights[i,]
      weightedWeight = weight$p*(weight$age/maxAge)
      temp = data.frame(filterId=fId, p=weightedWeight)
      vWeights = rbind(vWeights,temp)
    }
  }
}
#majority horizontal
hWeights = data.frame(filterId=NULL, p=NULL)
for(i in c(1:length(weights$filterId))){
  fId = weights$filterId[i]
  sub = subset(filters, filterId==fId)
  if((sub$y[1]==sub$y[2]) 
     || (sub$y[1]==sub$y[3])
     || (sub$y[2]==sub$y[3])
  ){
    if((sub$x[1]!=sub$x[2]) #exclude ambiguous orientations
       & (sub$x[1]!=sub$x[3])
       & (sub$x[2]!=sub$x[3])
    ){
      weight = weights[i,]
      weightedWeight = weight$p*(weight$age/maxAge)
      temp = data.frame(filterId=fId, p=weightedWeight)
      hWeights = rbind(hWeights,temp)
    }
  }
}
plot(vWeights$p, type="l", ylim = c(0,1))
print(mean(vWeights$p))
print(sqrt(var(vWeights$p)))
points(hWeights$p, type="l", col = "red")
print(mean(hWeights$p))
#standard devaiation
print(sqrt(var(hWeights$p)))
```

Plot weights in function of centering

```{r}

vWeightsCentering = centeringMatrixWeights(0)
plot(vWeightsCentering[1,], type="l", ylim = c(0,1), col="blue")
points(vWeightsCentering[2,], type="l", col="red")
points(vWeightsCentering[3,], type="l", col="black")

hWeightsCentering = centeringMatrixWeights(1)
plot(hWeightsCentering[1,], type="l", ylim = c(0,1), col="blue")
points(hWeightsCentering[2,], type="l", col="red")
points(hWeightsCentering[3,], type="l", col="black")



```


Functions

```{r}
#param orientation == 0 for vertical, 1 for horizontal centering
#return
#row = which filter-column we are considering
#column = number of cells in that filter-column
#value = p
centeringMatrixWeights = function(orientation){
  centering = matrix(0,filterSize,filterSize)
  counts = matrix(0,filterSize,filterSize)
  for(l in 1:length(weights$filterId)){
    fId = weights$filterId[l]
    sub = subset(filters, filterId==fId)
    #build matrix
    filter = matrix(0, filterSize, filterSize)
    for(i in 1:filterSize){
      if(orientation==0){
        filter[sub$x[i]+1,sub$y[i]+1] = 1 #inverted x and y to transpose filter
      } else {
        filter[sub$y[i]+1,sub$x[i]+1] = 1
      }
    }
    #sum row by row
    rSums = apply(filter,1,sum)
    #sum col by col
    cSums = apply(filter,2,sum)
    
    #exclude ambiguous orientations: majority alignment
    threshold = (filterSize/2)+0.5
    vInclude = cSums>=threshold
    hInclude = rSums>=threshold
    if(!any(vInclude)){
      if(any(hInclude)){
        weight = weights[l,]
        
        for(i in 1:length(rSums)){
          #i is the filter-column or filter-row, 
          # sums[i] is the number of cells belonging to it
          if(rSums[i]>0){
            weightedWeight = weight$p*(weight$age/maxAge)
            centering[i,rSums[i]] = centering[i,rSums[i]] + weightedWeight
            counts[i,rSums[i]] = counts[i,rSums[i]]+ 1
          }
        }
        weightsCentering = centering/counts
      }
    }
  }
  return(weightsCentering)
}
```

